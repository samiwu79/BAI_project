{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2893a5a",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This experiment is implemented using PyTorch within a dedicated Conda virtual environment (`oran311`) based on Python 3.11.\n",
    "\n",
    "The development environment includes:\n",
    "\n",
    "- Python 3.11\n",
    "- PyTorch 2.x (CPU version)\n",
    "- NumPy\n",
    "- Pandas\n",
    "- XGBoost\n",
    "\n",
    "\n",
    "The following cell verifies the installed PyTorch version and checks whether GPU acceleration (CUDA) is available.\n",
    "\n",
    "Since the current setup uses the CPU-only version of PyTorch, CUDA support is not enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e8a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca416237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10199\\anaconda3\\envs\\oran311\\python.exe\n",
      "Requirement already satisfied: pip in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (26.0.1)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.2.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (from xgboost) (2.4.2)\n",
      "Requirement already satisfied: scipy in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (from xgboost) (1.17.0)\n",
      "Using cached xgboost-3.2.0-py3-none-win_amd64.whl (101.7 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029a4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "print(\"xgboost version:\", xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f421623",
   "metadata": {},
   "source": [
    "## Model 3: Hybrid DNN–XGBoost (DNN Feature Extractor + XGBoost Regressor)\n",
    "\n",
    "This model follows the hybrid pipeline described in the Cam-Ready paper.  \n",
    "The key idea is to split the learning process into two stages:\n",
    "\n",
    "1) A DNN is trained as a **feature extractor** to learn compact latent representations.  \n",
    "2) The DNN is frozen, and a separate **XGBoost regressor** is trained on the extracted embeddings.\n",
    "\n",
    "### DNN Feature Extractor Architecture\n",
    "- Dense layers: 587 → 261 → 186 → 99\n",
    "- Bottleneck embedding layer: 16 neurons\n",
    "- Output head (for DNN training): 1 neuron (MSE loss)\n",
    "\n",
    "### XGBoost Regressor (trained on embeddings)\n",
    "- max_depth = 5\n",
    "- n_estimators = 256\n",
    "- learning_rate = 0.22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7b88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (4153, 3) (462, 3) (1154, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_ul_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\", \"txgain\"]\n",
    "\n",
    "target_col = \"pm_power\"\n",
    "\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "df = df[df[target_col] > 0].copy() \n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "\n",
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 3) scale (fit ONLY on train)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08a3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class HybridFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN feature extractor + a small regression head.\n",
    "    We train this end-to-end first, then freeze and use the 16-dim embeddings for XGBoost.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 587),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(587, 261),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(261, 186),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(186, 99),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(99, 16)   # bottleneck embeddings (paper uses 16)\n",
    "        )\n",
    "\n",
    "        # regression head for training the DNN stage\n",
    "        self.reg_head = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.feature_net(x)\n",
    "        out = self.reg_head(emb)\n",
    "        return out, emb\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_s)\n",
    "X_test_tensor  = torch.FloatTensor(X_test_s)\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_test_tensor  = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "input_dim = X_train_s.shape[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5821ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UL dataset (model3 - Hybrid DNN) training:\n",
      "Epoch 001 | train MSE 2.250002 | val MSE 2.081907\n",
      "Epoch 010 | train MSE 0.100358 | val MSE 0.089086\n",
      "Epoch 020 | train MSE 0.092565 | val MSE 0.084168\n",
      "Epoch 030 | train MSE 0.096473 | val MSE 0.086918\n",
      "Epoch 040 | train MSE 0.105854 | val MSE 0.100533\n",
      "Epoch 050 | train MSE 0.101289 | val MSE 0.091574\n",
      "Epoch 060 | train MSE 0.088105 | val MSE 0.080541\n",
      "Epoch 070 | train MSE 0.161307 | val MSE 0.159567\n",
      "Epoch 080 | train MSE 0.099074 | val MSE 0.091948\n",
      "Epoch 090 | train MSE 0.083177 | val MSE 0.077287\n",
      "Epoch 100 | train MSE 0.103379 | val MSE 0.100173\n",
      "Epoch 110 | train MSE 0.090587 | val MSE 0.087209\n",
      "Epoch 120 | train MSE 0.124728 | val MSE 0.122327\n",
      "Epoch 130 | train MSE 0.127948 | val MSE 0.126945\n",
      "Epoch 140 | train MSE 0.101011 | val MSE 0.094028\n",
      "Epoch 150 | train MSE 0.097970 | val MSE 0.095075\n",
      "Epoch 160 | train MSE 0.102417 | val MSE 0.094853\n",
      "Epoch 170 | train MSE 0.080623 | val MSE 0.076628\n",
      "Epoch 180 | train MSE 0.095710 | val MSE 0.091870\n",
      "Epoch 190 | train MSE 0.088915 | val MSE 0.084872\n",
      "Epoch 200 | train MSE 0.118761 | val MSE 0.116927\n",
      "Best val MSE: 0.07400397211313248\n",
      "Embeddings shape (train): (4153, 16)\n",
      "Embeddings shape (test) : (1154, 16)\n"
     ]
    }
   ],
   "source": [
    "model3_dnn = HybridFeatureExtractor(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model3_dnn.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "\n",
    "print(\"\\nUL dataset (model3 - Hybrid DNN) training:\")\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_s)\n",
    "y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model3_dnn.train()\n",
    "\n",
    "    perm = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        bx = X_train_tensor[idx]\n",
    "        by = y_train_tensor[idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model3_dnn(bx)\n",
    "        loss = criterion(pred, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- epoch-end evaluation ----\n",
    "    model3_dnn.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred, _ = model3_dnn(X_train_tensor)\n",
    "        val_pred, _   = model3_dnn(X_val_tensor)\n",
    "\n",
    "        train_mse = criterion(train_pred, y_train_tensor).item()\n",
    "        val_mse   = criterion(val_pred, y_val_tensor).item()\n",
    "\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "\n",
    "    # same printing style as model1\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1:03d} | train MSE {train_mse:.6f} | val MSE {val_mse:.6f}\")\n",
    "\n",
    "print(f\"Best val MSE: {best_val_mse}\")\n",
    "\n",
    "model3_dnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, emb_train = model3_dnn(X_train_tensor)\n",
    "    _, emb_test  = model3_dnn(X_test_tensor)\n",
    "\n",
    "emb_train = emb_train.numpy()\n",
    "emb_test  = emb_test.numpy()\n",
    "\n",
    "print(\"Embeddings shape (train):\", emb_train.shape)\n",
    "print(\"Embeddings shape (test) :\", emb_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7a933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 3: Hybrid DNN–XGBoost ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.106532\n",
      "RMSE : 0.326393\n",
      "MAE  : 0.226258\n",
      "MRE% : 1.9640\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    n_estimators=256,\n",
    "    learning_rate=0.22,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(emb_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(emb_test).reshape(-1)\n",
    "y_true = np.asarray(y_test).reshape(-1)\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred_xgb)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred_xgb)\n",
    "mre  = mean_relative_error(y_true, y_pred_xgb)\n",
    "\n",
    "print(\"\\n=== Model 3: Hybrid DNN–XGBoost ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4 — 方案B版 Model3 (Slice-wise) ✅ COPY-PASTE RUNNABLE\n",
    "# DNN Embedding + XGBoost(native ES, xgb.train)  | log1p target\n",
    "#\n",
    "# 方案B含义：\n",
    "#   - 按主变量（txgain / selected_mcs / airtime）的每个取值切片(slice)\n",
    "#   - 每个 slice 内随机 split: 80%train / 10%val(在train里) / 20%test\n",
    "#   - 模型输入默认只用“条件特征”(traffic_load, BW, nRBs, clockspeed)\n",
    "#   no feature engineering\n",
    "# 依赖已在内存中存在：\n",
    "#   - df (pd.DataFrame)\n",
    "#   - FEATURE_SETS (dict)  # 你原来的那份也可以\n",
    "#   - target_col (str)     # e.g. \"pm_power\" \n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Seed\n",
    "# ---------------------------\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Metrics\n",
    "# ---------------------------\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-3):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100.0)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mre = mean_relative_error(y_true, y_pred)\n",
    "    return {\"MSE\": float(mse), \"RMSE\": rmse, \"MAE\": float(mae), \"MRE(%)\": float(mre)}\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Clean numeric\n",
    "# ---------------------------\n",
    "def clean_numeric_df(df, cols_needed, target_col):\n",
    "    d = df.dropna(subset=cols_needed).copy()\n",
    "    for c in cols_needed:\n",
    "        d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "    d = d.dropna(subset=cols_needed).copy()\n",
    "    d = d[d[target_col] > 0].copy()\n",
    "    return d\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Dataset\n",
    "# ---------------------------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.asarray(X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.asarray(y), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ---------------------------\n",
    "# 4) DNN Feature Extractor\n",
    "# ---------------------------\n",
    "class HybridFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=16, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout/2),\n",
    "\n",
    "            nn.Linear(32, emb_dim),\n",
    "        )\n",
    "        self.reg_head = nn.Linear(emb_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.feature_net(x)\n",
    "        out = self.reg_head(emb)\n",
    "        return out, emb\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Train DNN + Extract Embeddings (log1p target)\n",
    "# ---------------------------\n",
    "def train_dnn_and_extract_embeddings(\n",
    "    X_train_s, y_train_log,\n",
    "    X_val_s,   y_val_log,\n",
    "    X_test_s,  y_test_log,\n",
    "    input_dim,\n",
    "    emb_dim=128,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=20,\n",
    "    min_delta=1e-6,\n",
    "    verbose_every=50,\n",
    "    seed=42\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    train_loader = DataLoader(TabularDataset(X_train_s, y_train_log), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TabularDataset(X_val_s,   y_val_log),   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(TabularDataset(X_test_s,  y_test_log),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = HybridFeatureExtractor(input_dim=input_dim, emb_dim=emb_dim).to(device)\n",
    "\n",
    "    loss_fn = nn.HuberLoss(delta=1.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=15)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_sum = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred, _ = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_sum += loss.item() * len(xb)\n",
    "        train_loss = train_sum / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred, _ = model(xb)\n",
    "                val_sum += loss_fn(pred, yb).item() * len(xb)\n",
    "        val_loss = val_sum / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val - min_delta:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if epoch == 1 or epoch % verbose_every == 0:\n",
    "            print(f\"  DNN Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | no_improve={no_improve}\")\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(f\"  DNN Early stop @ epoch {epoch} (patience={patience})\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    def extract_emb(loader):\n",
    "        model.eval()\n",
    "        embs, ys = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                _, emb = model(xb)\n",
    "                embs.append(emb.cpu().numpy())\n",
    "                ys.append(yb.numpy().reshape(-1))\n",
    "        return np.vstack(embs), np.concatenate(ys)\n",
    "\n",
    "    emb_train, y_train_log_1d = extract_emb(train_loader)\n",
    "    emb_val,   y_val_log_1d   = extract_emb(val_loader)\n",
    "    emb_test,  y_test_log_1d  = extract_emb(test_loader)\n",
    "\n",
    "    return model, emb_train, y_train_log_1d, emb_val, y_val_log_1d, emb_test, y_test_log_1d\n",
    "\n",
    "# ---------------------------\n",
    "# 6) XGBoost native ES (xgb.train)\n",
    "# ---------------------------\n",
    "def train_xgb_native_early_stopping(\n",
    "    emb_train, y_train_log,\n",
    "    emb_val,   y_val_log,\n",
    "    X_train_s=None,\n",
    "    X_val_s=None,\n",
    "    seed=42,\n",
    "    params=None,\n",
    "    num_boost_round=20000,\n",
    "    early_stopping_rounds=200\n",
    "):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": 0.01,\n",
    "            \"max_depth\": 5,\n",
    "            \"min_child_weight\": 3,\n",
    "            \"subsample\": 0.85,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"lambda\": 1.5,\n",
    "            \"alpha\": 0.5,\n",
    "            \"gamma\": 0.1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "\n",
    "    use_combined = (X_train_s is not None) and (X_val_s is not None)\n",
    "    if use_combined:\n",
    "        X_train = np.hstack([emb_train, X_train_s])\n",
    "        X_val   = np.hstack([emb_val,   X_val_s])\n",
    "    else:\n",
    "        X_train = emb_train\n",
    "        X_val   = emb_val\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_log)\n",
    "    dval   = xgb.DMatrix(X_val,   label=y_val_log)\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return booster, use_combined\n",
    "\n",
    "# ---------------------------\n",
    "# 7) One slice pipeline (方案B核心)\n",
    "# ---------------------------\n",
    "def train_eval_model3_one_slice(\n",
    "    d_slice,\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    seed=42,\n",
    "    # split\n",
    "    train_ratio=0.8,\n",
    "    val_ratio_within_train=0.1,\n",
    "    # DNN\n",
    "    emb_dim=16,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=20,\n",
    "    min_delta=1e-6,\n",
    "    verbose_every=50,\n",
    "    # XGB\n",
    "    xgb_params=None,\n",
    "    num_boost_round=500,\n",
    "    early_stopping_rounds=50,\n",
    "    # whether to concat raw scaled features with embedding\n",
    "    use_raw_plus_emb=True\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # split inside slice\n",
    "    d_train, d_test = train_test_split(d_slice, test_size=(1 - train_ratio), random_state=seed)\n",
    "    d_train, d_val  = train_test_split(d_train, test_size=val_ratio_within_train, random_state=seed)\n",
    "\n",
    "    X_train = d_train[feature_cols].values\n",
    "    X_val   = d_val[feature_cols].values\n",
    "    X_test  = d_test[feature_cols].values\n",
    "\n",
    "    y_train = d_train[target_col].values.astype(float)\n",
    "    y_val   = d_val[target_col].values.astype(float)\n",
    "    y_test  = d_test[target_col].values.astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s   = scaler.transform(X_val)\n",
    "    X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    y_val_log   = np.log1p(y_val)\n",
    "    y_test_log  = np.log1p(y_test)\n",
    "\n",
    "    # DNN -> embeddings\n",
    "    dnn, emb_train, y_train_log_1d, emb_val, y_val_log_1d, emb_test, y_test_log_1d = train_dnn_and_extract_embeddings(\n",
    "        X_train_s, y_train_log,\n",
    "        X_val_s,   y_val_log,\n",
    "        X_test_s,  y_test_log,\n",
    "        input_dim=len(feature_cols),\n",
    "        emb_dim=emb_dim,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        verbose_every=verbose_every,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # XGB\n",
    "    booster, used_combined = train_xgb_native_early_stopping(\n",
    "        emb_train, y_train_log_1d,\n",
    "        emb_val,   y_val_log_1d,\n",
    "        X_train_s=(X_train_s if use_raw_plus_emb else None),\n",
    "        X_val_s=(X_val_s if use_raw_plus_emb else None),\n",
    "        seed=seed,\n",
    "        params=xgb_params,\n",
    "        num_boost_round=num_boost_round,\n",
    "        early_stopping_rounds=early_stopping_rounds\n",
    "    )\n",
    "\n",
    "    # test predict\n",
    "    if used_combined:\n",
    "        X_test_for_xgb = np.hstack([emb_test, X_test_s])\n",
    "    else:\n",
    "        X_test_for_xgb = emb_test\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test_for_xgb)\n",
    "    y_pred_log = booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1)).reshape(-1)\n",
    "\n",
    "    y_pred = np.maximum(np.expm1(y_pred_log), 0.0)\n",
    "    y_true = np.expm1(y_test_log_1d)\n",
    "\n",
    "    metrics = compute_metrics(y_true, y_pred)\n",
    "\n",
    "    rmse_log = float(np.sqrt(mean_squared_error(y_test_log_1d, y_pred_log)))\n",
    "    approx_pct = float(np.expm1(rmse_log) * 100.0)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"rmse_log\": rmse_log,\n",
    "        \"approx_pct\": approx_pct,\n",
    "        \"n_train\": len(d_train),\n",
    "        \"n_val\": len(d_val),\n",
    "        \"n_test\": len(d_test),\n",
    "\n",
    "        \"d_train\": d_train.copy(),\n",
    "        \"d_val\": d_val.copy(),\n",
    "        \"d_test\": d_test.copy(),\n",
    "\n",
    "        \"dnn\": dnn,\n",
    "        \"booster\": booster,\n",
    "        \"scaler\": scaler,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 8) 方案B：按 slice 逐个训练 + 汇总\n",
    "# ============================================================\n",
    "\n",
    "# 默认：thesis/方案B常用“条件特征”作为输入（主变量用来分 slice，不放进输入）\n",
    "COND_FEATURES = [\"traffic_load\", \"BW\", \"nRBs\", \"clockspeed\"]\n",
    "COND_FEATURES = [c for c in COND_FEATURES if c in df.columns]\n",
    "\n",
    "# 你要模仿的“3个主变量实验”\n",
    "EXPERIMENTS = {\n",
    "    \"gain\": \"txgain\",\n",
    "    \"mcs\": \"selected_mcs\",\n",
    "    \"airtime\": \"airtime\",\n",
    "}\n",
    "\n",
    "# 如果你坚持“把主变量也作为输入特征”，改成 True（一般方案B不需要）\n",
    "INCLUDE_SLICE_IN_INPUT = False\n",
    "\n",
    "MIN_SLICE_SIZE = 30   # 每个 slice 至少多少样本才训练（太小会很不稳定）\n",
    "SEED = 42\n",
    "\n",
    "all_rows = []\n",
    "test_outputs_m3B = {}      # 你后面画图可用\n",
    "trained_models_m3B = {}    # 保存每个 slice 的模型\n",
    "\n",
    "for exp_name, slice_col in EXPERIMENTS.items():\n",
    "    if slice_col not in df.columns:\n",
    "        print(f\"[Skip] {exp_name}: slice_col '{slice_col}' not in df.columns\")\n",
    "        continue\n",
    "\n",
    "    # 输入特征\n",
    "    feature_cols = COND_FEATURES.copy()\n",
    "    if INCLUDE_SLICE_IN_INPUT and slice_col not in feature_cols:\n",
    "        feature_cols = [slice_col] + feature_cols\n",
    "\n",
    "    cols_needed = feature_cols + [slice_col, target_col]\n",
    "    d0 = clean_numeric_df(df, cols_needed, target_col)\n",
    "\n",
    "    print(\"\\n=====================================================\")\n",
    "    print(f\"[Model3] Experiment: {exp_name} | slice_col={slice_col}\")\n",
    "    print(f\"Input features: {feature_cols}\")\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    for sval, d_slice in d0.groupby(slice_col):\n",
    "        if len(d_slice) < MIN_SLICE_SIZE:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- slice {slice_col}={sval} | n={len(d_slice)} ---\")\n",
    "\n",
    "        out = train_eval_model3_one_slice(\n",
    "            d_slice=d_slice,\n",
    "            feature_cols=feature_cols,\n",
    "            target_col=target_col,\n",
    "            seed=SEED,\n",
    "            train_ratio=0.8,\n",
    "            val_ratio_within_train=0.1,\n",
    "            emb_dim=128,\n",
    "            epochs=600,\n",
    "            batch_size=128,\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-4,\n",
    "            patience=60,\n",
    "            min_delta=1e-6,\n",
    "            verbose_every=100,\n",
    "            xgb_params=None,\n",
    "            num_boost_round=20000,\n",
    "            early_stopping_rounds=200,\n",
    "            use_raw_plus_emb=True\n",
    "        )\n",
    "\n",
    "        m = out[\"metrics\"]\n",
    "\n",
    "        all_rows.append({\n",
    "            \"experiment\": exp_name,\n",
    "            \"slice_col\": slice_col,\n",
    "            \"slice_value\": sval,\n",
    "            \"features\": \",\".join(feature_cols),\n",
    "            \"MSE\": m[\"MSE\"],\n",
    "            \"RMSE\": m[\"RMSE\"],\n",
    "            \"MAE\": m[\"MAE\"],\n",
    "            \"MRE(%)\": m[\"MRE(%)\"],\n",
    "            \"RMSE_log\": out[\"rmse_log\"],\n",
    "            \"approx_rel_err(%)\": out[\"approx_pct\"],\n",
    "            \"n_train\": out[\"n_train\"],\n",
    "            \"n_val\": out[\"n_val\"],\n",
    "            \"n_test\": out[\"n_test\"],\n",
    "            \"n_slice\": len(d_slice),\n",
    "        })\n",
    "\n",
    "        # 保存测试输出/模型（按 experiment + slice_value）\n",
    "        test_outputs_m3B.setdefault(exp_name, {})\n",
    "        test_outputs_m3B[exp_name][sval] = {\n",
    "            \"test_df\": out[\"d_test\"].copy(),\n",
    "            \"y_true\": out[\"y_true\"],\n",
    "            \"y_pred_m3B\": out[\"y_pred\"]\n",
    "        }\n",
    "\n",
    "        trained_models_m3B.setdefault(exp_name, {})\n",
    "        trained_models_m3B[exp_name][sval] = {\n",
    "            \"dnn\": out[\"dnn\"],\n",
    "            \"booster\": out[\"booster\"],\n",
    "            \"scaler\": out[\"scaler\"],\n",
    "            \"feature_cols\": feature_cols\n",
    "        }\n",
    "\n",
    "# per-slice results\n",
    "results_slices_df = pd.DataFrame(all_rows)\n",
    "print(\"\\n===== Model3: per-slice Results =====\")\n",
    "display(results_slices_df.sort_values([\"experiment\", \"slice_value\"]))\n",
    "\n",
    "# weighted summary per experiment (按 n_test 加权)\n",
    "if len(results_slices_df) > 0:\n",
    "    summary_rows = []\n",
    "    for exp_name in results_slices_df[\"experiment\"].unique():\n",
    "        sub = results_slices_df[results_slices_df[\"experiment\"] == exp_name].copy()\n",
    "        w = sub[\"n_test\"].values.astype(float)\n",
    "        w = np.maximum(w, 1.0)\n",
    "        def wavg(col):\n",
    "            return float(np.sum(sub[col].values * w) / np.sum(w))\n",
    "        summary_rows.append({\n",
    "            \"experiment\": exp_name,\n",
    "            \"n_slices\": int(len(sub)),\n",
    "            \"RMSE_wavg\": wavg(\"RMSE\"),\n",
    "            \"MAE_wavg\":  wavg(\"MAE\"),\n",
    "            \"MRE_wavg(%)\": wavg(\"MRE(%)\"),\n",
    "            \"MSE_wavg\":  wavg(\"MSE\"),\n",
    "        })\n",
    "    results_summary_df = pd.DataFrame(summary_rows).sort_values(\"MRE_wavg(%)\")\n",
    "    print(\"\\n===== Model3:Summary (weighted by n_test) =====\")\n",
    "    display(results_summary_df)\n",
    "else:\n",
    "    print(\"\\n[Warn] empty\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oran311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
