{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2893a5a",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This experiment is implemented using PyTorch within a dedicated Conda virtual environment (`oran311`) based on Python 3.11.\n",
    "\n",
    "The development environment includes:\n",
    "\n",
    "- Python 3.11\n",
    "- PyTorch 2.x (CPU version)\n",
    "- NumPy\n",
    "- Pandas\n",
    "- XGBoost\n",
    "\n",
    "\n",
    "The following cell verifies the installed PyTorch version and checks whether GPU acceleration (CUDA) is available.\n",
    "\n",
    "Since the current setup uses the CPU-only version of PyTorch, CUDA support is not enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e8a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca416237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10199\\anaconda3\\envs\\oran311\\python.exe\n",
      "Requirement already satisfied: pip in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (26.0.1)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.2.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (from xgboost) (2.4.2)\n",
      "Requirement already satisfied: scipy in c:\\Users\\10199\\anaconda3\\envs\\oran311\\Lib\\site-packages (from xgboost) (1.17.0)\n",
      "Using cached xgboost-3.2.0-py3-none-win_amd64.whl (101.7 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029a4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "print(\"xgboost version:\", xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f421623",
   "metadata": {},
   "source": [
    "## Model 3: Hybrid DNN–XGBoost (DNN Feature Extractor + XGBoost Regressor)\n",
    "\n",
    "This model follows the hybrid pipeline described in the Cam-Ready paper.  \n",
    "The key idea is to split the learning process into two stages:\n",
    "\n",
    "1) A DNN is trained as a **feature extractor** to learn compact latent representations.  \n",
    "2) The DNN is frozen, and a separate **XGBoost regressor** is trained on the extracted embeddings.\n",
    "\n",
    "### DNN Feature Extractor Architecture\n",
    "- Dense layers: 587 → 261 → 186 → 99\n",
    "- Bottleneck embedding layer: 16 neurons\n",
    "- Output head (for DNN training): 1 neuron (MSE loss)\n",
    "\n",
    "### XGBoost Regressor (trained on embeddings)\n",
    "- max_depth = 5\n",
    "- n_estimators = 256\n",
    "- learning_rate = 0.22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7b88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (4153, 3) (462, 3) (1154, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_ul_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\", \"txgain\"]\n",
    "\n",
    "target_col = \"pm_power\"\n",
    "\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "df = df[df[target_col] > 0].copy() \n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "\n",
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 3) scale (fit ONLY on train)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08a3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class HybridFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN feature extractor + a small regression head.\n",
    "    We train this end-to-end first, then freeze and use the 16-dim embeddings for XGBoost.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 587),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(587, 261),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(261, 186),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(186, 99),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(99, 16)   # bottleneck embeddings (paper uses 16)\n",
    "        )\n",
    "\n",
    "        # regression head for training the DNN stage\n",
    "        self.reg_head = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.feature_net(x)\n",
    "        out = self.reg_head(emb)\n",
    "        return out, emb\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_s)\n",
    "X_test_tensor  = torch.FloatTensor(X_test_s)\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_test_tensor  = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "input_dim = X_train_s.shape[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5821ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UL dataset (model3 - Hybrid DNN) training:\n",
      "Epoch 001 | train MSE 2.250002 | val MSE 2.081907\n",
      "Epoch 010 | train MSE 0.100358 | val MSE 0.089086\n",
      "Epoch 020 | train MSE 0.092565 | val MSE 0.084168\n",
      "Epoch 030 | train MSE 0.096473 | val MSE 0.086918\n",
      "Epoch 040 | train MSE 0.105854 | val MSE 0.100533\n",
      "Epoch 050 | train MSE 0.101289 | val MSE 0.091574\n",
      "Epoch 060 | train MSE 0.088105 | val MSE 0.080541\n",
      "Epoch 070 | train MSE 0.161307 | val MSE 0.159567\n",
      "Epoch 080 | train MSE 0.099074 | val MSE 0.091948\n",
      "Epoch 090 | train MSE 0.083177 | val MSE 0.077287\n",
      "Epoch 100 | train MSE 0.103379 | val MSE 0.100173\n",
      "Epoch 110 | train MSE 0.090587 | val MSE 0.087209\n",
      "Epoch 120 | train MSE 0.124728 | val MSE 0.122327\n",
      "Epoch 130 | train MSE 0.127948 | val MSE 0.126945\n",
      "Epoch 140 | train MSE 0.101011 | val MSE 0.094028\n",
      "Epoch 150 | train MSE 0.097970 | val MSE 0.095075\n",
      "Epoch 160 | train MSE 0.102417 | val MSE 0.094853\n",
      "Epoch 170 | train MSE 0.080623 | val MSE 0.076628\n",
      "Epoch 180 | train MSE 0.095710 | val MSE 0.091870\n",
      "Epoch 190 | train MSE 0.088915 | val MSE 0.084872\n",
      "Epoch 200 | train MSE 0.118761 | val MSE 0.116927\n",
      "Best val MSE: 0.07400397211313248\n",
      "Embeddings shape (train): (4153, 16)\n",
      "Embeddings shape (test) : (1154, 16)\n"
     ]
    }
   ],
   "source": [
    "model3_dnn = HybridFeatureExtractor(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model3_dnn.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "\n",
    "print(\"\\nUL dataset (model3 - Hybrid DNN) training:\")\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_s)\n",
    "y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model3_dnn.train()\n",
    "\n",
    "    perm = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        bx = X_train_tensor[idx]\n",
    "        by = y_train_tensor[idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model3_dnn(bx)\n",
    "        loss = criterion(pred, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- epoch-end evaluation ----\n",
    "    model3_dnn.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred, _ = model3_dnn(X_train_tensor)\n",
    "        val_pred, _   = model3_dnn(X_val_tensor)\n",
    "\n",
    "        train_mse = criterion(train_pred, y_train_tensor).item()\n",
    "        val_mse   = criterion(val_pred, y_val_tensor).item()\n",
    "\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "\n",
    "    # same printing style as model1\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1:03d} | train MSE {train_mse:.6f} | val MSE {val_mse:.6f}\")\n",
    "\n",
    "print(f\"Best val MSE: {best_val_mse}\")\n",
    "\n",
    "model3_dnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, emb_train = model3_dnn(X_train_tensor)\n",
    "    _, emb_test  = model3_dnn(X_test_tensor)\n",
    "\n",
    "emb_train = emb_train.numpy()\n",
    "emb_test  = emb_test.numpy()\n",
    "\n",
    "print(\"Embeddings shape (train):\", emb_train.shape)\n",
    "print(\"Embeddings shape (test) :\", emb_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7a933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 3: Hybrid DNN–XGBoost ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.106532\n",
      "RMSE : 0.326393\n",
      "MAE  : 0.226258\n",
      "MRE% : 1.9640\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    n_estimators=256,\n",
    "    learning_rate=0.22,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(emb_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(emb_test).reshape(-1)\n",
    "y_true = np.asarray(y_test).reshape(-1)\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred_xgb)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred_xgb)\n",
    "mre  = mean_relative_error(y_true, y_pred_xgb)\n",
    "\n",
    "print(\"\\n=== Model 3: Hybrid DNN–XGBoost ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oran311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
