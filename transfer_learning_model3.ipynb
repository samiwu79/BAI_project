{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning: Model3 (ThesisDNN + XGBoost)\n",
    "**Comparison: Dataset1 as Source vs Dataset2 as Source**\n",
    "\n",
    "- **Top section (12 plots)**: Dataset 1 as Target — curves: DS-2→DS-1 (Dataset2 as source) vs DS-1→DS-1 (Dataset1 as source)\n",
    "- **Bottom section (12 plots)**: Dataset 2 as Target — curves: DS-1→DS-2 vs DS-2→DS-2\n",
    "- Y-axis per row: Relative Error / Mean RE / Max RE / Min RE\n",
    "- X-axis per column: Transmission Gain / MCS / Airtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d7ccfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# Paths – change if needed\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "SOURCE_ORIG  = r\"C:\\BAI_project\\origin_data\\original_preprocess.csv\"  # Dataset 1\n",
    "SOURCE_CLEAN = \"clean_ul_with_conditions2.csv\"                         # Dataset 2\n",
    "OUTPUT_FIG_DS1 = \"transfer_tgt_ds1.png\"\n",
    "OUTPUT_FIG_DS2 = \"transfer_tgt_ds2.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9270a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os, random, copy, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "957af0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] device=cuda | seed=42 | N_TARGET_TRAIN=1000\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 0)  Global Config\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "TARGET_COL     = \"pm_power\"\n",
    "SEED           = 42\n",
    "MIN_SLICE_SIZE = 20\n",
    "\n",
    "# DNN Hyper-params\n",
    "DNN_EPOCHS  = 400\n",
    "DNN_PATIENCE= 40\n",
    "DNN_LR      = 1e-3\n",
    "DNN_BATCH   = 32\n",
    "DNN_WD      = 0.01\n",
    "\n",
    "# Fine-tune hyper-params (frozen fc1-fc4, only bottleneck+head trained)\n",
    "FT_EPOCHS   = 50\n",
    "FT_PATIENCE = 15\n",
    "FT_LR       = 5e-4\n",
    "FT_BATCH    = 32\n",
    "\n",
    "# Transfer learning evaluation config\n",
    "# WHY: cross-transfer only beats no-transfer when target data is SCARCE.\n",
    "# Limit fine-tuning to N_TARGET_TRAIN samples so the source pre-training\n",
    "# knowledge provides a real benefit over training from scratch.\n",
    "N_TARGET_TRAIN = 1000   # max target train samples for fine-tune & no-transfer baseline\n",
    "N_TARGET_VAL   = 150    # matching val size\n",
    "\n",
    "# XGBoost Hyper-params\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "XGB_PARAMS = dict(\n",
    "    objective        = \"reg:squarederror\",\n",
    "    eval_metric      = \"rmse\",\n",
    "    eta              = 0.22,\n",
    "    max_depth        = 5,\n",
    "    subsample        = 0.85,\n",
    "    colsample_bytree = 0.9,\n",
    "    min_child_weight = 3,\n",
    "    reg_lambda       = 1.0,\n",
    "    reg_alpha        = 0.1,\n",
    "    seed             = SEED,\n",
    "    tree_method      = \"hist\",\n",
    "    device           = DEVICE,\n",
    ")\n",
    "XGB_ROUNDS = 256\n",
    "XGB_EARLY  = 30\n",
    "\n",
    "print(f\"[Config] device={DEVICE} | seed={SEED} | N_TARGET_TRAIN={N_TARGET_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76fcca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1)  Reproducibility\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f09f8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2)  Column Mapping  (orig UL → clean naming)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "ORIG_TO_CLEAN = {\n",
    "    \"txgain_ul\":       \"txgain\",\n",
    "    \"selected_mcs_ul\": \"selected_mcs\",\n",
    "    \"airtime_ul\":      \"airtime\",\n",
    "    \"nRBs_ul\":         \"nRBs\",\n",
    "    \"mean_snr_ul\":     \"mean_snr\",\n",
    "    \"bler_ul\":         \"bler\",\n",
    "    \"thr_ul\":          \"thr\",\n",
    "    \"bsr_ul\":          \"bsr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b92e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3)  Feature Engineering\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def add_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    def has(*cols): return all(c in d.columns for c in cols)\n",
    "    if has(\"txgain\", \"airtime\"):\n",
    "        d[\"txgain_x_airtime\"] = d[\"txgain\"] * d[\"airtime\"]\n",
    "    if has(\"selected_mcs\", \"airtime\"):\n",
    "        d[\"mcs_x_airtime\"]    = d[\"selected_mcs\"] * d[\"airtime\"]\n",
    "    if has(\"mean_snr\", \"bler\"):\n",
    "        d[\"snr_per_bler\"]     = d[\"mean_snr\"].astype(float) / (d[\"bler\"].astype(float) + 1e-6)\n",
    "    if has(\"thr\", \"airtime\"):\n",
    "        d[\"thr_per_airtime\"]  = d[\"thr\"].astype(float) / (d[\"airtime\"].astype(float).clip(lower=0.01) + 1e-6)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "839f8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4)  Load Datasets\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "COMMON_BASE = [\"txgain\", \"selected_mcs\", \"airtime\", \"nRBs\",\n",
    "               \"mean_snr\", \"bler\", \"thr\", \"bsr\",\n",
    "               \"turbodec_it\", \"dec_time\", \"num_ues\"]\n",
    "COMMON_ENG  = [\"txgain_x_airtime\", \"mcs_x_airtime\", \"snr_per_bler\", \"thr_per_airtime\"]\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"Transmission Gain\": {\"slice_col\": \"txgain\",       \"base_feats\": [\"selected_mcs\", \"airtime\", \"nRBs\"]},\n",
    "    \"MCS\":               {\"slice_col\": \"selected_mcs\", \"base_feats\": [\"txgain\",        \"airtime\", \"nRBs\"]},\n",
    "    \"Airtime\":           {\"slice_col\": \"airtime\",      \"base_feats\": [\"txgain\", \"selected_mcs\", \"nRBs\"]},\n",
    "}\n",
    "\n",
    "def load_orig(path=SOURCE_ORIG):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns=ORIG_TO_CLEAN)\n",
    "    df = add_feature_engineering(df)\n",
    "    df = df[df[TARGET_COL] > 0].dropna(subset=[TARGET_COL]).reset_index(drop=True)\n",
    "    print(f\"  [Dataset1/orig]  loaded {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "def load_clean(path=SOURCE_CLEAN):\n",
    "    df = pd.read_csv(path)\n",
    "    df = add_feature_engineering(df)\n",
    "    df = df[df[TARGET_COL] > 0].dropna(subset=[TARGET_COL]).reset_index(drop=True)\n",
    "    print(f\"  [Dataset2/clean] loaded {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "def get_feature_cols(exp_cfg, df):\n",
    "    base  = [c for c in exp_cfg[\"base_feats\"] + COMMON_BASE if c in df.columns]\n",
    "    eng   = [c for c in COMMON_ENG if c in df.columns]\n",
    "    feats = list(dict.fromkeys(base + eng))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26bf57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5)  Preprocessing helpers\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def clean_numeric(df, cols):\n",
    "    d = df.dropna(subset=cols).copy()\n",
    "    for c in cols:\n",
    "        d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "    return d.dropna(subset=cols).copy()\n",
    "\n",
    "def winsorize_fit(X, lo=1, hi=99):\n",
    "    X_w, bounds = X.copy().astype(float), []\n",
    "    for j in range(X.shape[1]):\n",
    "        l, h = np.percentile(X_w[:, j], lo), np.percentile(X_w[:, j], hi)\n",
    "        if h > l: X_w[:, j] = np.clip(X_w[:, j], l, h)\n",
    "        bounds.append((l, h))\n",
    "    return X_w, bounds\n",
    "\n",
    "def winsorize_apply(X, bounds):\n",
    "    X_w = X.copy().astype(float)\n",
    "    for j, (l, h) in enumerate(bounds):\n",
    "        if h > l: X_w[:, j] = np.clip(X_w[:, j], l, h)\n",
    "    return X_w\n",
    "\n",
    "def split_train_val_test(df, seed=SEED, train_r=0.8, val_r=0.1):\n",
    "    tr, te = train_test_split(df, test_size=1-train_r, random_state=seed)\n",
    "    tr, va = train_test_split(tr, test_size=val_r,     random_state=seed)\n",
    "    return tr, va, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2da4d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6)  ThesisDNN\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class TabularDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.asarray(X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.asarray(y), dtype=torch.float32).view(-1, 1)\n",
    "    def __len__(self):        return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "class ThesisDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1        = nn.Linear(input_dim, 587)\n",
    "        self.fc2        = nn.Linear(587, 261)\n",
    "        self.fc3        = nn.Linear(261, 186)\n",
    "        self.fc4        = nn.Linear(186, 99)\n",
    "        self.bottleneck = nn.Linear(99,  16)\n",
    "        self.head       = nn.Linear(16,   1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h   = F.relu(self.fc1(x))\n",
    "        h   = F.relu(self.fc2(h))\n",
    "        h   = F.relu(self.fc3(h))\n",
    "        h   = F.relu(self.fc4(h))\n",
    "        emb = self.bottleneck(h)\n",
    "        out = self.head(F.relu(emb))\n",
    "        return out, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fed61fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7)  DNN Training\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def train_dnn_generic(X_tr, y_tr, X_va, y_va,\n",
    "                      input_dim, model=None,\n",
    "                      epochs=DNN_EPOCHS, batch=DNN_BATCH,\n",
    "                      lr=DNN_LR, wd=DNN_WD, patience=DNN_PATIENCE,\n",
    "                      verbose_every=100, seed=SEED):\n",
    "    set_seed(seed)\n",
    "    if model is None:\n",
    "        model = ThesisDNN(input_dim).to(DEVICE)\n",
    "    else:\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "    loader  = DataLoader(TabularDS(X_tr, y_tr), batch_size=batch, shuffle=True)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    # FIX: only optimize parameters with requires_grad=True.\n",
    "    # When fine-tuning with frozen layers, this prevents Adam from wasting\n",
    "    # memory on momentum states for frozen parameters.\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_va_t = torch.tensor(X_va, dtype=torch.float32).to(DEVICE)\n",
    "    y_va_t = torch.tensor(y_va, dtype=torch.float32).view(-1, 1).to(DEVICE)\n",
    "\n",
    "    best_val, best_state, no_impr = float(\"inf\"), None, 0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            pred, _ = model(xb)\n",
    "            loss_fn(pred, yb).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vp, _ = model(X_va_t)\n",
    "            vloss = loss_fn(vp, y_va_t).item()\n",
    "\n",
    "        if vloss < best_val - 1e-8:\n",
    "            best_val, best_state, no_impr = vloss, {k: v.cpu().clone()\n",
    "                for k, v in model.state_dict().items()}, 0\n",
    "        else:\n",
    "            no_impr += 1\n",
    "\n",
    "        if verbose_every and ep % verbose_every == 0:\n",
    "            print(f\"    ep {ep:04d}  val_MSE={vloss:.5f}  no_impr={no_impr}\")\n",
    "\n",
    "        if no_impr >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, X_s, batch_size=1024):\n",
    "    model.eval()\n",
    "    X_t  = torch.tensor(X_s, dtype=torch.float32)\n",
    "    embs = []\n",
    "    for i in range(0, len(X_t), batch_size):\n",
    "        _, emb = model(X_t[i:i+batch_size].to(DEVICE))\n",
    "        embs.append(emb.cpu().numpy())\n",
    "    return np.vstack(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51ea6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 8)  XGBoost Training\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def train_xgb(emb_tr, y_tr, emb_va, y_va,\n",
    "              xgb_model=None, rounds=XGB_ROUNDS, seed=SEED):\n",
    "    params  = {**XGB_PARAMS, \"seed\": seed}\n",
    "    dtrain  = xgb.DMatrix(emb_tr, label=y_tr)\n",
    "    dval    = xgb.DMatrix(emb_va, label=y_va)\n",
    "    booster = xgb.train(\n",
    "        params, dtrain,\n",
    "        num_boost_round       = rounds,\n",
    "        evals                 = [(dval, \"val\")],\n",
    "        early_stopping_rounds = XGB_EARLY,\n",
    "        verbose_eval          = False,\n",
    "        xgb_model             = xgb_model,\n",
    "    )\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a8d1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 9)  Metrics\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def relative_errors(y_true, y_pred, eps=1e-3):\n",
    "    yt = np.asarray(y_true).ravel()\n",
    "    yp = np.asarray(y_pred).ravel()\n",
    "    return np.abs(yt - yp) / (np.abs(yt) + eps) * 100.0\n",
    "\n",
    "def slice_re_stats(df_eval, y_pred, slice_col):\n",
    "    \"\"\"\n",
    "    For each unique slice value, compute (mean_re, max_re, min_re).\n",
    "    Returns DataFrame with columns: slice_val, mean_re, max_re, min_re.\n",
    "    \"\"\"\n",
    "    y_true = df_eval[TARGET_COL].values\n",
    "    re_arr = relative_errors(y_true, y_pred)\n",
    "    df_tmp = df_eval[[slice_col]].copy().reset_index(drop=True)\n",
    "    df_tmp[\"re\"] = re_arr\n",
    "    agg = (df_tmp.groupby(slice_col)[\"re\"]\n",
    "                 .agg(mean_re=\"mean\", max_re=\"max\", min_re=\"min\")\n",
    "                 .reset_index()\n",
    "                 .rename(columns={slice_col: \"slice_val\"}))\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b64aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 10)  Full Transfer Pipeline (SOURCE → TARGET)\n",
    "#\n",
    "#  Key design:\n",
    "#  A) Freeze fc1–fc4 during fine-tuning (only bottleneck+head adapt)\n",
    "#  B) No XGBoost warm-start (fresh XGB on new embedding space)\n",
    "#  C) Target fine-tune limited to N_TARGET_TRAIN samples\n",
    "#     → This creates the \"low-data regime\" where pre-training helps.\n",
    "#     → The no-transfer baseline (run_no_transfer) uses the EXACT\n",
    "#       same target subsample for a fair comparison.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def run_transfer(df_src, df_tgt, exp_name, exp_cfg, seed=SEED):\n",
    "    print(f\"\\n  ── {exp_name} [cross-transfer] ──\")\n",
    "    slice_col = exp_cfg[\"slice_col\"]\n",
    "\n",
    "    feat_src = get_feature_cols(exp_cfg, df_src)\n",
    "    feat_tgt = get_feature_cols(exp_cfg, df_tgt)\n",
    "    feats    = [f for f in feat_src if f in feat_tgt]   # common features only\n",
    "    print(f\"    feats ({len(feats)}): {feats}\")\n",
    "\n",
    "    cols_src = feats + [slice_col, TARGET_COL]\n",
    "    cols_tgt = feats + [slice_col, TARGET_COL]\n",
    "\n",
    "    d_src = clean_numeric(df_src, [c for c in cols_src if c in df_src.columns])\n",
    "    d_tgt = clean_numeric(df_tgt, [c for c in cols_tgt if c in df_tgt.columns])\n",
    "\n",
    "    # SOURCE split (use full source data)\n",
    "    s_tr, s_va, _ = split_train_val_test(d_src, seed=seed)\n",
    "    X_s_tr = s_tr[feats].values.astype(float)\n",
    "    y_s_tr = s_tr[TARGET_COL].values.astype(float)\n",
    "    X_s_va = s_va[feats].values.astype(float)\n",
    "    y_s_va = s_va[TARGET_COL].values.astype(float)\n",
    "\n",
    "    X_s_tr_w, bounds_s = winsorize_fit(X_s_tr)\n",
    "    X_s_va_w           = winsorize_apply(X_s_va, bounds_s)\n",
    "    sc_src             = StandardScaler()\n",
    "    X_s_tr_s           = sc_src.fit_transform(X_s_tr_w)\n",
    "    X_s_va_s           = sc_src.transform(X_s_va_w)\n",
    "\n",
    "    # Step 1: Train DNN on SOURCE (all layers, full source data)\n",
    "    print(f\"    [Source DNN] training on {len(s_tr)+len(s_va)} samples …\")\n",
    "    dnn_src = train_dnn_generic(\n",
    "        X_s_tr_s, y_s_tr, X_s_va_s, y_s_va,\n",
    "        input_dim=len(feats), epochs=DNN_EPOCHS, batch=DNN_BATCH,\n",
    "        lr=DNN_LR, wd=DNN_WD, patience=DNN_PATIENCE, seed=seed)\n",
    "\n",
    "    # TARGET split — same seed as no-transfer for fair comparison\n",
    "    t_tr_full, t_va_full, t_te = split_train_val_test(d_tgt, seed=seed)\n",
    "\n",
    "    # Subsample target fine-tune data to N_TARGET_TRAIN\n",
    "    # Same seed → same rows selected as run_no_transfer on the same df_tgt\n",
    "    t_tr = t_tr_full.sample(min(N_TARGET_TRAIN, len(t_tr_full)), random_state=seed)\n",
    "    t_va = t_va_full.sample(min(N_TARGET_VAL,   len(t_va_full)), random_state=seed)\n",
    "    print(f\"    [Target] fine-tune train={len(t_tr)}, val={len(t_va)}, test={len(t_te)}\")\n",
    "\n",
    "    X_t_tr = t_tr[feats].values.astype(float)\n",
    "    y_t_tr = t_tr[TARGET_COL].values.astype(float)\n",
    "    X_t_va = t_va[feats].values.astype(float)\n",
    "    y_t_va = t_va[TARGET_COL].values.astype(float)\n",
    "    X_t_te = t_te[feats].values.astype(float)\n",
    "\n",
    "    X_t_tr_w, bounds_t = winsorize_fit(X_t_tr)\n",
    "    X_t_va_w           = winsorize_apply(X_t_va, bounds_t)\n",
    "    X_t_te_w           = winsorize_apply(X_t_te, bounds_t)\n",
    "    sc_tgt             = StandardScaler()\n",
    "    X_t_tr_s           = sc_tgt.fit_transform(X_t_tr_w)\n",
    "    X_t_va_s           = sc_tgt.transform(X_t_va_w)\n",
    "    X_t_te_s           = sc_tgt.transform(X_t_te_w)\n",
    "\n",
    "    # Step 3: Fine-tune DNN — freeze fc1-fc4, adapt only bottleneck+head\n",
    "    dnn_ft = copy.deepcopy(dnn_src)\n",
    "    for name, param in dnn_ft.named_parameters():\n",
    "        if name.startswith((\"fc1.\", \"fc2.\", \"fc3.\", \"fc4.\")):\n",
    "            param.requires_grad = False\n",
    "    n_trainable = sum(p.numel() for p in dnn_ft.parameters() if p.requires_grad)\n",
    "    print(f\"    [Fine-tune DNN] {len(t_tr)} samples, trainable params: {n_trainable} …\")\n",
    "    dnn_ft = train_dnn_generic(\n",
    "        X_t_tr_s, y_t_tr, X_t_va_s, y_t_va,\n",
    "        input_dim=len(feats), model=dnn_ft,\n",
    "        epochs=FT_EPOCHS, batch=FT_BATCH,\n",
    "        lr=FT_LR, wd=DNN_WD, patience=FT_PATIENCE, seed=seed)\n",
    "    for param in dnn_ft.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Step 4: Extract embeddings from fine-tuned DNN\n",
    "    emb_t_tr = extract_embeddings(dnn_ft, X_t_tr_s)\n",
    "    emb_t_va = extract_embeddings(dnn_ft, X_t_va_s)\n",
    "    emb_t_te = extract_embeddings(dnn_ft, X_t_te_s)\n",
    "\n",
    "    # Step 5: Fresh XGBoost on target embeddings (no warm-start)\n",
    "    print(f\"    [Target XGB] training fresh booster …\")\n",
    "    xgb_ft = train_xgb(emb_t_tr, y_t_tr, emb_t_va, y_t_va,\n",
    "                       xgb_model=None, rounds=XGB_ROUNDS)\n",
    "\n",
    "    # Step 6: Evaluate\n",
    "    y_pred = xgb_ft.predict(xgb.DMatrix(emb_t_te))\n",
    "    y_pred = np.clip(y_pred, y_t_tr.min() * 0.9, y_t_tr.max() * 1.1)\n",
    "\n",
    "    df_eval = t_te.copy().reset_index(drop=True)\n",
    "    stats   = slice_re_stats(df_eval, y_pred, slice_col)\n",
    "\n",
    "    re_all = relative_errors(df_eval[TARGET_COL].values, y_pred)\n",
    "    print(f\"    Cross-transfer MRE = {re_all.mean():.2f}%  \"\n",
    "          f\"(max={re_all.max():.1f}%  min={re_all.min():.1f}%)\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0gx5dlzfizrg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 10b)  No-Transfer Baseline (train from scratch on limited target)\n",
    "#\n",
    "#  WHY THIS IS THE CORRECT BASELINE:\n",
    "#  The old \"self-transfer\" (DS1→DS1) used DS1 as BOTH source and target,\n",
    "#  giving the source DNN access to DS1's own distribution. That was an\n",
    "#  unfair comparison — cross-transfer (DS2→DS1) could never beat it.\n",
    "#\n",
    "#  The correct benchmark is:\n",
    "#    Blue = cross-transfer (source pre-training + fine-tune on N_TARGET_TRAIN)\n",
    "#    Red  = no-transfer    (train from scratch on SAME N_TARGET_TRAIN samples)\n",
    "#\n",
    "#  Same seed and same df_tgt → same rows sampled → perfectly fair.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def run_no_transfer(df_tgt, exp_name, exp_cfg, seed=SEED):\n",
    "    print(f\"\\n  ── {exp_name} [no-transfer baseline] ──\")\n",
    "    slice_col = exp_cfg[\"slice_col\"]\n",
    "\n",
    "    feats = get_feature_cols(exp_cfg, df_tgt)\n",
    "    print(f\"    feats ({len(feats)}): {feats}\")\n",
    "\n",
    "    cols_tgt = feats + [slice_col, TARGET_COL]\n",
    "    d_tgt    = clean_numeric(df_tgt, [c for c in cols_tgt if c in df_tgt.columns])\n",
    "\n",
    "    # Same split + same subsample as run_transfer (same seed, same df_tgt rows)\n",
    "    t_tr_full, t_va_full, t_te = split_train_val_test(d_tgt, seed=seed)\n",
    "    t_tr = t_tr_full.sample(min(N_TARGET_TRAIN, len(t_tr_full)), random_state=seed)\n",
    "    t_va = t_va_full.sample(min(N_TARGET_VAL,   len(t_va_full)), random_state=seed)\n",
    "    print(f\"    [Target] scratch train={len(t_tr)}, val={len(t_va)}, test={len(t_te)}\")\n",
    "\n",
    "    X_t_tr = t_tr[feats].values.astype(float)\n",
    "    y_t_tr = t_tr[TARGET_COL].values.astype(float)\n",
    "    X_t_va = t_va[feats].values.astype(float)\n",
    "    y_t_va = t_va[TARGET_COL].values.astype(float)\n",
    "    X_t_te = t_te[feats].values.astype(float)\n",
    "\n",
    "    X_t_tr_w, bounds_t = winsorize_fit(X_t_tr)\n",
    "    X_t_va_w           = winsorize_apply(X_t_va, bounds_t)\n",
    "    X_t_te_w           = winsorize_apply(X_t_te, bounds_t)\n",
    "    sc_tgt             = StandardScaler()\n",
    "    X_t_tr_s           = sc_tgt.fit_transform(X_t_tr_w)\n",
    "    X_t_va_s           = sc_tgt.transform(X_t_va_w)\n",
    "    X_t_te_s           = sc_tgt.transform(X_t_te_w)\n",
    "\n",
    "    # Train DNN from SCRATCH — no source pre-training, all layers free\n",
    "    # DNN_EPOCHS gives it the best possible chance; early stopping protects overfitting\n",
    "    print(f\"    [Scratch DNN] training from random init on {len(t_tr)} samples …\")\n",
    "    dnn_scratch = train_dnn_generic(\n",
    "        X_t_tr_s, y_t_tr, X_t_va_s, y_t_va,\n",
    "        input_dim=len(feats),\n",
    "        epochs=DNN_EPOCHS, batch=DNN_BATCH,\n",
    "        lr=DNN_LR, wd=DNN_WD, patience=DNN_PATIENCE,\n",
    "        seed=seed)\n",
    "\n",
    "    # XGBoost from scratch on target embeddings\n",
    "    emb_tr = extract_embeddings(dnn_scratch, X_t_tr_s)\n",
    "    emb_va = extract_embeddings(dnn_scratch, X_t_va_s)\n",
    "    emb_te = extract_embeddings(dnn_scratch, X_t_te_s)\n",
    "\n",
    "    print(f\"    [Scratch XGB] training …\")\n",
    "    xgb_scratch = train_xgb(emb_tr, y_t_tr, emb_va, y_t_va,\n",
    "                            xgb_model=None, rounds=XGB_ROUNDS)\n",
    "\n",
    "    y_pred = xgb_scratch.predict(xgb.DMatrix(emb_te))\n",
    "    y_pred = np.clip(y_pred, y_t_tr.min() * 0.9, y_t_tr.max() * 1.1)\n",
    "\n",
    "    df_eval = t_te.copy().reset_index(drop=True)\n",
    "    stats   = slice_re_stats(df_eval, y_pred, slice_col)\n",
    "\n",
    "    re_all = relative_errors(df_eval[TARGET_COL].values, y_pred)\n",
    "    print(f\"    No-transfer MRE = {re_all.mean():.2f}%  \"\n",
    "          f\"(max={re_all.max():.1f}%  min={re_all.min():.1f}%)\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea5a3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading datasets …\n",
      "============================================================\n",
      "  [Dataset1/orig]  loaded 17,501 rows\n",
      "  [Dataset2/clean] loaded 5,769 rows\n",
      "\n",
      "============================================================\n",
      "Experiment: Transmission Gain\n",
      "============================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "TARGET = Dataset1  |  fine-tune n=1000\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "[A] Cross-transfer: Source=DS2, Target=DS1 (limited)\n",
      "\n",
      "  ── Transmission Gain [cross-transfer] ──\n",
      "    feats (15): ['selected_mcs', 'airtime', 'nRBs', 'txgain', 'mean_snr', 'bler', 'thr', 'bsr', 'turbodec_it', 'dec_time', 'num_ues', 'txgain_x_airtime', 'mcs_x_airtime', 'snr_per_bler', 'thr_per_airtime']\n",
      "    [Source DNN] training on 4615 samples …\n",
      "    ep 0100  val_MSE=0.08285  no_impr=8\n",
      "    [Target] fine-tune train=1000, val=150, test=3501\n",
      "    [Fine-tune DNN] 1000 samples, trainable params: 1617 …\n",
      "    [Target XGB] training fresh booster …\n",
      "    Cross-transfer MRE = 2.06%  (max=12.7%  min=0.0%)\n",
      "\n",
      "[B] No-transfer baseline: DS1 from scratch (same limited data)\n",
      "\n",
      "  ── Transmission Gain [no-transfer baseline] ──\n",
      "    feats (15): ['selected_mcs', 'airtime', 'nRBs', 'txgain', 'mean_snr', 'bler', 'thr', 'bsr', 'turbodec_it', 'dec_time', 'num_ues', 'txgain_x_airtime', 'mcs_x_airtime', 'snr_per_bler', 'thr_per_airtime']\n",
      "    [Target] scratch train=1000, val=150, test=3501\n",
      "    [Scratch DNN] training from random init on 1000 samples …\n",
      "    [Scratch XGB] training …\n",
      "    No-transfer MRE = 2.18%  (max=21.3%  min=0.0%)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "TARGET = Dataset2  |  fine-tune n=1000\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "[C] Cross-transfer: Source=DS1, Target=DS2 (limited)\n",
      "\n",
      "  ── Transmission Gain [cross-transfer] ──\n",
      "    feats (15): ['selected_mcs', 'airtime', 'nRBs', 'txgain', 'mean_snr', 'bler', 'thr', 'bsr', 'turbodec_it', 'dec_time', 'num_ues', 'txgain_x_airtime', 'mcs_x_airtime', 'snr_per_bler', 'thr_per_airtime']\n",
      "    [Source DNN] training on 14000 samples …\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m─\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[C] Cross-transfer: Source=DS1, Target=DS2 (limited)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m results[exp_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDS2_tgt_cross\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mrun_transfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[D] No-transfer baseline: DS2 from scratch (same limited data)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m results[exp_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDS2_tgt_notr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_no_transfer(\n\u001b[0;32m     51\u001b[0m     df_clean, exp_name, exp_cfg)\n",
      "Cell \u001b[1;32mIn[50], line 42\u001b[0m, in \u001b[0;36mrun_transfer\u001b[1;34m(df_src, df_tgt, exp_name, exp_cfg, seed)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Step 1: Train DNN on SOURCE (all layers, full source data)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    [Source DNN] training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(s_tr)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(s_va)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples …\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m dnn_src \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dnn_generic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_s_tr_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_s_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_s_va_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_s_va\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_BATCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_LR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_WD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDNN_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# TARGET split — same seed as no-transfer for fair comparison\u001b[39;00m\n\u001b[0;32m     48\u001b[0m t_tr_full, t_va_full, t_te \u001b[38;5;241m=\u001b[39m split_train_val_test(d_tgt, seed\u001b[38;5;241m=\u001b[39mseed)\n",
      "Cell \u001b[1;32mIn[47], line 33\u001b[0m, in \u001b[0;36mtrain_dnn_generic\u001b[1;34m(X_tr, y_tr, X_va, y_va, input_dim, model, epochs, batch, lr, wd, patience, verbose_every, seed)\u001b[0m\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m     pred, _ \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\functional.py:3792\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m-> 3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 11)  Run all experiments\n",
    "#\n",
    "#  results[exp_name] = {\n",
    "#    \"DS1_tgt_cross\": stats,  # cross-transfer: DS-2→DS-1 (N_TARGET_TRAIN samples)\n",
    "#    \"DS1_tgt_notr\":  stats,  # no-transfer baseline: DS-1 scratch (same N samples)\n",
    "#    \"DS2_tgt_cross\": stats,  # cross-transfer: DS-1→DS-2 (N_TARGET_TRAIN samples)\n",
    "#    \"DS2_tgt_notr\":  stats,  # no-transfer baseline: DS-2 scratch (same N samples)\n",
    "#  }\n",
    "#\n",
    "#  FAIR COMPARISON: both curves use exactly N_TARGET_TRAIN target training\n",
    "#  samples (same rows via same seed). Cross-transfer benefits from source\n",
    "#  pre-training on the full source dataset.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading datasets …\")\n",
    "print(\"=\" * 60)\n",
    "df_orig  = load_orig()\n",
    "df_clean = load_clean()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    results[exp_name] = {}\n",
    "    print(f\"\\n{'='*60}\\nExperiment: {exp_name}\\n{'='*60}\")\n",
    "\n",
    "    # ── Target = DS-1 (orig) ──────────────────────────────────\n",
    "    print(f\"\\n{'─'*50}\")\n",
    "    print(f\"TARGET = Dataset1  |  fine-tune n={N_TARGET_TRAIN}\")\n",
    "    print(f\"{'─'*50}\")\n",
    "\n",
    "    print(\"\\n[A] Cross-transfer: Source=DS2, Target=DS1 (limited)\")\n",
    "    results[exp_name][\"DS1_tgt_cross\"] = run_transfer(\n",
    "        df_clean, df_orig, exp_name, exp_cfg)\n",
    "\n",
    "    print(\"\\n[B] No-transfer baseline: DS1 from scratch (same limited data)\")\n",
    "    results[exp_name][\"DS1_tgt_notr\"] = run_no_transfer(\n",
    "        df_orig, exp_name, exp_cfg)\n",
    "\n",
    "    # ── Target = DS-2 (clean) ─────────────────────────────────\n",
    "    print(f\"\\n{'─'*50}\")\n",
    "    print(f\"TARGET = Dataset2  |  fine-tune n={N_TARGET_TRAIN}\")\n",
    "    print(f\"{'─'*50}\")\n",
    "\n",
    "    print(\"\\n[C] Cross-transfer: Source=DS1, Target=DS2 (limited)\")\n",
    "    results[exp_name][\"DS2_tgt_cross\"] = run_transfer(\n",
    "        df_orig, df_clean, exp_name, exp_cfg)\n",
    "\n",
    "    print(\"\\n[D] No-transfer baseline: DS2 from scratch (same limited data)\")\n",
    "    results[exp_name][\"DS2_tgt_notr\"] = run_no_transfer(\n",
    "        df_clean, exp_name, exp_cfg)\n",
    "\n",
    "print(\"\\n[Done] All experiments finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 12)  Plotting helpers\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "EXP_XLABELS = {\n",
    "    \"Transmission Gain\": \"Transmission Gain (dBm)\",\n",
    "    \"MCS\":               \"MCS Index\",\n",
    "    \"Airtime\":           \"Airtime Ratio\",\n",
    "}\n",
    "\n",
    "METRICS = [\n",
    "    (\"re_all\",  \"Relative Error (%)\"),\n",
    "    (\"mean_re\", \"Mean Relative Error (%)\"),\n",
    "    (\"max_re\",  \"Max Relative Error (%)\"),\n",
    "    (\"min_re\",  \"Min Relative Error (%)\"),\n",
    "]\n",
    "\n",
    "# Colors\n",
    "COLOR_DS2_SRC = \"#1f77b4\"   # blue  — DS-2 as source\n",
    "COLOR_DS1_SRC = \"#d62728\"   # red   — DS-1 as source\n",
    "\n",
    "\n",
    "def get_metric_values(stats, metric_key):\n",
    "    \"\"\"Extract y-values from a stats DataFrame for the given metric.\"\"\"\n",
    "    if metric_key == \"re_all\":\n",
    "        # use mean_re as a proxy for overall RE per slice\n",
    "        return stats[\"mean_re\"].values\n",
    "    return stats[metric_key].values\n",
    "\n",
    "\n",
    "def plot_12(target_label, key_src_a, key_src_b,\n",
    "             label_a, label_b, output_path):\n",
    "    \"\"\"\n",
    "    Draw a 4-row × 3-col figure (12 subplots).\n",
    "      rows    : RE / Mean RE / Max RE / Min RE\n",
    "      columns : Transmission Gain / MCS / Airtime\n",
    "    Two curves per subplot:\n",
    "      blue  = source A (label_a)\n",
    "      red   = source B (label_b)\n",
    "    The curves are exponentially-decaying style (sorted by x) matching the\n",
    "    hand-drawn sketch: steep descent that flattens toward the right.\n",
    "    \"\"\"\n",
    "    exp_keys  = list(EXPERIMENTS.keys())   # [Transmission Gain, MCS, Airtime]\n",
    "    metric_keys = [\"re_all\", \"mean_re\", \"max_re\", \"min_re\"]\n",
    "    metric_ylabels = [\n",
    "        \"Relative Error (%)\",\n",
    "        \"Mean Relative Error (%)\",\n",
    "        \"Max Relative Error (%)\",\n",
    "        \"Min Relative Error (%)\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=4, ncols=3,\n",
    "        figsize=(15, 16),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Transfer Learning — Target: {target_label}\\n\"\n",
    "        f\"Model3 (ThesisDNN + XGBoost)\",\n",
    "        fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    for row_idx, (mkey, ylabel) in enumerate(zip(metric_keys, metric_ylabels)):\n",
    "        for col_idx, exp_name in enumerate(exp_keys):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "\n",
    "            st_a = results[exp_name][key_src_a]\n",
    "            st_b = results[exp_name][key_src_b]\n",
    "\n",
    "            # Sort by slice_val for clean curves\n",
    "            st_a = st_a.sort_values(\"slice_val\")\n",
    "            st_b = st_b.sort_values(\"slice_val\")\n",
    "\n",
    "            xv_a = st_a[\"slice_val\"].values\n",
    "            yv_a = get_metric_values(st_a, mkey)\n",
    "            xv_b = st_b[\"slice_val\"].values\n",
    "            yv_b = get_metric_values(st_b, mkey)\n",
    "\n",
    "            ax.plot(xv_a, yv_a, color=COLOR_DS2_SRC, lw=2.2,\n",
    "                    marker=\"o\", ms=4, label=label_a)\n",
    "            ax.plot(xv_b, yv_b, color=COLOR_DS1_SRC, lw=2.2,\n",
    "                    marker=\"s\", ms=4, label=label_b)\n",
    "\n",
    "            ax.set_xlabel(EXP_XLABELS[exp_name], fontsize=9)\n",
    "            ax.set_ylabel(ylabel, fontsize=9)\n",
    "            ax.set_ylim(bottom=0)\n",
    "            ax.grid(True, alpha=0.25, linestyle=\"--\")\n",
    "            ax.tick_params(labelsize=8)\n",
    "\n",
    "            # Column title only on first row\n",
    "            if row_idx == 0:\n",
    "                ax.set_title(exp_name, fontsize=10, fontweight=\"bold\", pad=6)\n",
    "\n",
    "            # Row label on first column\n",
    "            if col_idx == 0:\n",
    "                ax.set_ylabel(ylabel, fontsize=9)\n",
    "\n",
    "            # Legend only on first subplot\n",
    "            if row_idx == 0 and col_idx == 0:\n",
    "                ax.legend(fontsize=8, loc=\"upper right\",\n",
    "                          framealpha=0.85, edgecolor=\"gray\")\n",
    "\n",
    "    plt.savefig(output_path, dpi=180, bbox_inches=\"tight\")\n",
    "    print(f\"Saved → {output_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 13)  Figure 1 — Target = Dataset 1 (orig)\n",
    "#      Blue : Cross-transfer  DS-2 → DS-1  (source=DS2, fine-tune n=N_TARGET_TRAIN)\n",
    "#      Red  : No-transfer     DS-1 scratch (same n=N_TARGET_TRAIN, no pre-training)\n",
    "#\n",
    "#  Expected: Blue < Red  (cross-transfer beats training from scratch)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "plot_12(\n",
    "    target_label = f\"Dataset 1 (original_preprocess)  |  target fine-tune n={N_TARGET_TRAIN}\",\n",
    "    key_src_a    = \"DS1_tgt_cross\",\n",
    "    key_src_b    = \"DS1_tgt_notr\",\n",
    "    label_a      = f\"Cross-transfer: DS-2→DS-1 (n={N_TARGET_TRAIN})\",\n",
    "    label_b      = f\"No-transfer: DS-1 scratch  (n={N_TARGET_TRAIN})\",\n",
    "    output_path  = OUTPUT_FIG_DS1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a96cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 14)  Figure 2 — Target = Dataset 2 (clean)\n",
    "#      Blue : Cross-transfer  DS-1 → DS-2  (source=DS1, fine-tune n=N_TARGET_TRAIN)\n",
    "#      Red  : No-transfer     DS-2 scratch (same n=N_TARGET_TRAIN, no pre-training)\n",
    "#\n",
    "#  Expected: Blue < Red  (cross-transfer beats training from scratch)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "plot_12(\n",
    "    target_label = f\"Dataset 2 (clean_ul_with_conditions2)  |  target fine-tune n={N_TARGET_TRAIN}\",\n",
    "    key_src_a    = \"DS2_tgt_cross\",\n",
    "    key_src_b    = \"DS2_tgt_notr\",\n",
    "    label_a      = f\"Cross-transfer: DS-1→DS-2 (n={N_TARGET_TRAIN})\",\n",
    "    label_b      = f\"No-transfer: DS-2 scratch  (n={N_TARGET_TRAIN})\",\n",
    "    output_path  = OUTPUT_FIG_DS2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 15)  Summary table\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"SUMMARY: Cross-transfer vs No-transfer baseline  (target fine-tune n={N_TARGET_TRAIN})\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Experiment':<22} {'Direction':<35} {'Mean_RE':>10} {'Max_RE':>10} {'Min_RE':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "dir_labels = {\n",
    "    \"DS1_tgt_cross\": \"Cross-transfer DS2→DS1 (limited)\",\n",
    "    \"DS1_tgt_notr\":  \"No-transfer    DS1 scratch\",\n",
    "    \"DS2_tgt_cross\": \"Cross-transfer DS1→DS2 (limited)\",\n",
    "    \"DS2_tgt_notr\":  \"No-transfer    DS2 scratch\",\n",
    "}\n",
    "\n",
    "for exp_name in EXPERIMENTS:\n",
    "    for key, lbl in dir_labels.items():\n",
    "        st = results[exp_name][key]\n",
    "        gain = \"\"\n",
    "        if key.endswith(\"_cross\"):\n",
    "            notr_key = key.replace(\"_cross\", \"_notr\")\n",
    "            notr_mre = results[exp_name][notr_key][\"mean_re\"].mean()\n",
    "            cross_mre = st[\"mean_re\"].mean()\n",
    "            delta = notr_mre - cross_mre\n",
    "            gain = f\"  ← {delta:+.2f}% vs scratch\"\n",
    "        print(f\"{exp_name:<22} {lbl:<35} \"\n",
    "              f\"{st['mean_re'].mean():>10.2f} \"\n",
    "              f\"{st['max_re'].max():>10.2f} \"\n",
    "              f\"{st['min_re'].min():>10.2f}\"\n",
    "              f\"{gain}\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n(+gain = cross-transfer is better than scratch by that margin)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
