{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcc7005",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This experiment is implemented using PyTorch. The following cell verifies the PyTorch version and whether GPU acceleration is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66236ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5818ecd",
   "metadata": {},
   "source": [
    "### First Part: O-RAN Data Loading and Pre-processing\n",
    "\n",
    "The dataset is loaded from a pre-cleaned CSV file. Only two core system-level features are used as model inputs:\n",
    "- `airtime`\n",
    "- `selected_mcs`\n",
    "\n",
    "The target variable is `pm_power`, representing power consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b981237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_oran_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\"]\n",
    "target_col = \"pm_power\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a88c6",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Rows containing missing or non-numeric values in the selected features or target variable are removed to ensure data consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c80c3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31618cc8",
   "metadata": {},
   "source": [
    "### Dataset Split\n",
    "\n",
    "The dataset is split into training, validation, and test sets:\n",
    "- 80% training + test split\n",
    "- 10% of the training set is further used as a validation set\n",
    "\n",
    "This ensures that model selection is performed using unseen validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c025b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd37ab",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "All input features are standardised using `StandardScaler`.  \n",
    "The scaler is fitted only on the training data and then applied to validation and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a525aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (12600, 2) (1400, 2) (3501, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3) scale \n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff303b3",
   "metadata": {},
   "source": [
    "### PyTorch Dataset and DataLoader\n",
    "\n",
    "A custom `Dataset` class is defined to convert the tabular data into PyTorch tensors.  \n",
    "Mini-batches are generated using `DataLoader` with a batch size of 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acaa9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_s, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TabularDataset(X_val_s, y_val), batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(TabularDataset(X_test_s, y_test), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11316923",
   "metadata": {},
   "source": [
    "### Baseline DNN Model\n",
    "\n",
    "The baseline model is a fully-connected feed-forward neural network with the following architecture:\n",
    "\n",
    "Input → 64 → 64 → 32 → Output\n",
    "\n",
    "ReLU activations are applied after each hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ccee2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineDNN(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4168ae",
   "metadata": {},
   "source": [
    "### Training Setup\n",
    "\n",
    "The model is trained using:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: Mean Squared Error (MSE)\n",
    "- Number of epochs: 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bbb1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineDNN(in_dim=len(feature_cols)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e4e62",
   "metadata": {},
   "source": [
    "### Model Training and Validation\n",
    "\n",
    "During training, both training loss and validation loss are monitored.  \n",
    "The model state corresponding to the lowest validation MSE is saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa1feb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train MSE 33.489325 | val MSE 0.829133\n",
      "Epoch 010 | train MSE 0.112074 | val MSE 0.106964\n",
      "Epoch 020 | train MSE 0.112689 | val MSE 0.108245\n",
      "Epoch 030 | train MSE 0.114906 | val MSE 0.105963\n",
      "Epoch 040 | train MSE 0.112826 | val MSE 0.123028\n",
      "Epoch 050 | train MSE 0.116505 | val MSE 0.121863\n",
      "Epoch 060 | train MSE 0.114099 | val MSE 0.107916\n",
      "Epoch 070 | train MSE 0.115754 | val MSE 0.106424\n",
      "Epoch 080 | train MSE 0.116262 | val MSE 0.116174\n",
      "Epoch 090 | train MSE 0.115124 | val MSE 0.104782\n",
      "Epoch 100 | train MSE 0.116261 | val MSE 0.107666\n",
      "Best val MSE: 0.10456097441060203\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * len(xb)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            val_loss += loss_fn(pred, yb).item() * len(xb)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.6f} | val MSE {val_loss:.6f}\")\n",
    "\n",
    "# load best\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Best val MSE:\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045808e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "The trained model is evaluated on the test set using the following metrics:\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Relative Error (MRE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61eed150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== O-RAN Baseline DNN (A→A) ===\n",
      "X: ['airtime', 'selected_mcs']  y: pm_power\n",
      "MSE  : 0.105108\n",
      "RMSE : 0.324203\n",
      "MAE  : 0.248304\n",
      "MRE% : 1.8522\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy().reshape(-1)\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(yb.numpy().reshape(-1))\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mre  = mean_relative_error(y_true, y_pred)\n",
    "\n",
    "print(\"=== O-RAN Baseline DNN (A→A) ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e1402",
   "metadata": {},
   "source": [
    "### Second Part: UL Data Loading and Pre-processing\n",
    "\n",
    "The dataset is loaded from a pre-cleaned CSV file. Only two core system-level features are used as model inputs:\n",
    "- `airtime`\n",
    "- `selected_mcs`\n",
    "\n",
    "The target variable is `pm_power`, representing power consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e7fba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (13614, 2) (1513, 2) (3782, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_ul_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\"]\n",
    "target_col = \"pm_power\"\n",
    "\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 3) scale (fit ONLY on train)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e734b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train MSE 599.258520 | val MSE 447.883639\n",
      "Epoch 010 | train MSE 437.008393 | val MSE 436.685690\n",
      "Epoch 020 | train MSE 436.539886 | val MSE 438.393185\n",
      "Epoch 030 | train MSE 436.987887 | val MSE 436.509275\n",
      "Epoch 040 | train MSE 436.725238 | val MSE 435.929388\n",
      "Epoch 050 | train MSE 436.224979 | val MSE 437.521136\n",
      "Epoch 060 | train MSE 436.033513 | val MSE 436.274244\n",
      "Epoch 070 | train MSE 435.921746 | val MSE 437.216786\n",
      "Epoch 080 | train MSE 435.735534 | val MSE 435.816170\n",
      "Epoch 090 | train MSE 435.355527 | val MSE 435.682704\n",
      "Epoch 100 | train MSE 436.563635 | val MSE 434.662530\n",
      "Best val MSE: 434.5573084012452\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class BaselineDNN(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_s, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TabularDataset(X_val_s, y_val), batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(TabularDataset(X_test_s, y_test), batch_size=64, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineDNN(in_dim=len(feature_cols)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * len(xb)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            val_loss += loss_fn(pred, yb).item() * len(xb)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.6f} | val MSE {val_loss:.6f}\")\n",
    "\n",
    "# load best\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Best val MSE:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== O-RAN Baseline DNN (A→A) ===\n",
      "X: ['airtime', 'selected_mcs']  y: pm_power\n",
      "MSE  : 442.947174\n",
      "RMSE : 21.046310\n",
      "MAE  : 18.635468\n",
      "MRE% : 97.9922\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy().reshape(-1)\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(yb.numpy().reshape(-1))\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mre  = mean_relative_error(y_true, y_pred)\n",
    "\n",
    "print(\"=== UL Baseline DNN (B→B) ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968cf1d",
   "metadata": {},
   "source": [
    "\n",
    "## Baseline Results on UL Dataset (B→B)\n",
    "\n",
    "The baseline DNN model was first trained and evaluated on the UL dataset using the same feature set (`airtime`, `selected_mcs`) and network architecture as in the O-RAN baseline experiments. This was done to ensure a fair and controlled comparison between the two datasets.\n",
    "\n",
    "The training and validation losses decrease smoothly during training and remain close to each other, indicating that the model is able to converge stably and does not suffer from overfitting. However, the absolute error values remain high throughout training, and the final test performance is poor, with a mean relative error (MRE) of approximately 90%.\n",
    "\n",
    "This result suggests that the poor performance is not caused by optimisation issues or model instability, but rather by the limited predictive power of the selected input features for the UL dataset.\n",
    "\n",
    "### Analysis of UL Power Characteristics\n",
    "\n",
    "Further analysis of the UL dataset reveals a highly multi-modal distribution of power consumption. In contrast to the O-RAN dataset, where power values are concentrated within a narrow range, the UL dataset exhibits several distinct power levels, spanning from low-power states (~10 W) to high-power states (>60 W).\n",
    "\n",
    "The UL dataset exhibits a multi-modal power distribution, indicating the presence of multiple underlying system states. As airtime alone cannot distinguish between these states, baseline regression models fail to capture the power dynamics, resulting in high prediction error.\n",
    "\n",
    "Scatter plots of power consumption against airtime show that, for a fixed airtime value, the observed power consumption can vary significantly across multiple operating regimes. This indicates the presence of multiple underlying system states (e.g., different platform configurations, background load conditions, or processing modes) that are not explicitly captured by the available features.\n",
    "\n",
    "As a result, the mapping from (`airtime`, `selected_mcs`) to power consumption in the UL dataset is inherently ambiguous. Under such conditions, a regression model trained from random initialisation is unable to identify a single consistent relationship, leading to large prediction errors despite stable convergence.\n",
    "\n",
    "### Motivation for Transfer Learning\n",
    "\n",
    "Although the baseline model performs poorly on the UL dataset, this does not imply that the UL data itself is unsuitable for modelling. Instead, it highlights that the UL task is more challenging and that the available features alone are insufficient to fully characterise the power consumption behaviour.\n",
    "\n",
    "In contrast, the baseline results on the O-RAN dataset demonstrate that the same feature set can effectively predict power consumption in a related but more controlled system. This suggests that the model trained on O-RAN data has already learned physically meaningful representations of how wireless transmission parameters influence system power consumption.\n",
    "\n",
    "Transfer learning therefore provides a natural next step. By initialising the UL model with parameters learned from the O-RAN dataset, the model is no longer trained from scratch, but instead starts from a representation that encodes general power–parameter relationships observed in a related domain. This prior knowledge can help guide learning on the UL dataset, potentially reducing prediction error even when the target data exhibits multiple operating regimes and limited feature informativeness.\n",
    "\n",
    "Based on these observations, the next stage of this work investigates whether transfer learning from O-RAN to UL can improve power prediction performance compared to training solely on the UL dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oran311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
