{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcc7005",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This experiment is implemented using PyTorch. The following cell verifies the PyTorch version and whether GPU acceleration is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66236ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5818ecd",
   "metadata": {},
   "source": [
    "### First Part: O-RAN Data Loading and Pre-processing\n",
    "\n",
    "The dataset is loaded from a pre-cleaned CSV file. Only two core system-level features are used as model inputs:\n",
    "- `airtime`\n",
    "- `selected_mcs`\n",
    "\n",
    "The target variable is `pm_power`, representing power consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b981237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_oran_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\", \"txgain\"]\n",
    "\n",
    "target_col = \"pm_power\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a88c6",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Rows containing missing or non-numeric values in the selected features or target variable are removed to ensure data consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c80c3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31618cc8",
   "metadata": {},
   "source": [
    "### Dataset Split\n",
    "\n",
    "The dataset is split into training, validation, and test sets:\n",
    "- 80% training + test split\n",
    "- 10% of the training set is further used as a validation set\n",
    "\n",
    "This ensures that model selection is performed using unseen validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c025b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd37ab",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "All input features are standardised using `StandardScaler`.  \n",
    "The scaler is fitted only on the training data and then applied to validation and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a525aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (12600, 3) (1400, 3) (3501, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3) scale \n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff303b3",
   "metadata": {},
   "source": [
    "### PyTorch Dataset and DataLoader\n",
    "\n",
    "A custom `Dataset` class is defined to convert the tabular data into PyTorch tensors.  \n",
    "Mini-batches are generated using `DataLoader` with a batch size of 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acaa9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_s, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TabularDataset(X_val_s, y_val), batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(TabularDataset(X_test_s, y_test), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11316923",
   "metadata": {},
   "source": [
    "### Baseline DNN Model\n",
    "\n",
    "The baseline model is a fully-connected feed-forward neural network with the following architecture:\n",
    "\n",
    "Input → 64 → 64 → 32 → Output\n",
    "\n",
    "ReLU activations are applied after each hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ccee2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineDNN(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4168ae",
   "metadata": {},
   "source": [
    "### Training Setup\n",
    "\n",
    "The model is trained using:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: Mean Squared Error (MSE)\n",
    "- Number of epochs: 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bbb1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineDNN(in_dim=len(feature_cols)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e4e62",
   "metadata": {},
   "source": [
    "### Model Training and Validation\n",
    "\n",
    "During training, both training loss and validation loss are monitored.  \n",
    "The model state corresponding to the lowest validation MSE is saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa1feb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oran traning:\n",
      "Epoch 001 | train MSE 40.054616 | val MSE 2.745626\n",
      "Epoch 010 | train MSE 0.111275 | val MSE 0.109863\n",
      "Epoch 020 | train MSE 0.109722 | val MSE 0.109653\n",
      "Epoch 030 | train MSE 0.109607 | val MSE 0.105094\n",
      "Epoch 040 | train MSE 0.108029 | val MSE 0.108676\n",
      "Epoch 050 | train MSE 0.106111 | val MSE 0.101157\n",
      "Epoch 060 | train MSE 0.105163 | val MSE 0.104319\n",
      "Epoch 070 | train MSE 0.109655 | val MSE 0.108392\n",
      "Epoch 080 | train MSE 0.108350 | val MSE 0.104790\n",
      "Epoch 090 | train MSE 0.106436 | val MSE 0.119163\n",
      "Epoch 100 | train MSE 0.105336 | val MSE 0.101477\n",
      "Best val MSE: 0.1001981965984617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Oran traning:\")\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * len(xb)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            val_loss += loss_fn(pred, yb).item() * len(xb)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.6f} | val MSE {val_loss:.6f}\")\n",
    "\n",
    "# load best\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Best val MSE:\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045808e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "The trained model is evaluated on the test set using the following metrics:\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Relative Error (MRE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61eed150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== O-RAN Baseline DNN (A→A) ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.098468\n",
      "RMSE : 0.313796\n",
      "MAE  : 0.242035\n",
      "MRE% : 1.8081\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy().reshape(-1)\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(yb.numpy().reshape(-1))\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mre  = mean_relative_error(y_true, y_pred)\n",
    "\n",
    "print(\"=== O-RAN Baseline DNN (A→A) ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e1402",
   "metadata": {},
   "source": [
    "### Second Part: UL Data Loading and Pre-processing\n",
    "\n",
    "The dataset is loaded from a pre-cleaned CSV file. Only two core system-level features are used as model inputs:\n",
    "- `airtime`\n",
    "- `selected_mcs`\n",
    "\n",
    "The target variable is `pm_power`, representing power consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7fba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (4153, 3) (462, 3) (1154, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1) load data\n",
    "df = pd.read_csv(\"clean_ul_stage1.csv\")\n",
    "\n",
    "feature_cols = [\"airtime\", \"selected_mcs\", \"txgain\"]\n",
    "\n",
    "target_col = \"pm_power\"\n",
    "\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "for c in feature_cols + [target_col]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
    "df = df[df[target_col] > 0].copy() \n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "\n",
    "# 2) split: train/test then train/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 3) scale (fit ONLY on train)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b80a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UL test size: 1154\n"
     ]
    }
   ],
   "source": [
    "# ====== UL: One shared tensor set for all models ======\n",
    "X_train_tensor = torch.FloatTensor(X_train_s)\n",
    "X_val_tensor   = torch.FloatTensor(X_val_s)\n",
    "X_test_tensor  = torch.FloatTensor(X_test_s)\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_val_tensor   = torch.FloatTensor(y_val).view(-1, 1)\n",
    "y_test_tensor  = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "y_true_ul = y_test_tensor.numpy().flatten()  \n",
    "print(\"UL test size:\", len(y_true_ul))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e734b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UL dataset (model1) training:\n",
      "Epoch 001 | train MSE 62.313203 | val MSE 4.820665\n",
      "Epoch 010 | train MSE 0.114848 | val MSE 0.102119\n",
      "Epoch 020 | train MSE 0.107342 | val MSE 0.092927\n",
      "Epoch 030 | train MSE 0.104289 | val MSE 0.091414\n",
      "Epoch 040 | train MSE 0.102170 | val MSE 0.090550\n",
      "Epoch 050 | train MSE 0.102935 | val MSE 0.091630\n",
      "Epoch 060 | train MSE 0.102253 | val MSE 0.090480\n",
      "Epoch 070 | train MSE 0.100085 | val MSE 0.096231\n",
      "Epoch 080 | train MSE 0.104938 | val MSE 0.093121\n",
      "Epoch 090 | train MSE 0.098333 | val MSE 0.086488\n",
      "Epoch 100 | train MSE 0.100583 | val MSE 0.087265\n",
      "Best val MSE: 0.08552217438365474\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class BaselineDNN(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_s, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TabularDataset(X_val_s, y_val), batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(TabularDataset(X_test_s, y_test), batch_size=64, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineDNN(in_dim=len(feature_cols)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "print(\"UL dataset (model1) training:\")\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * len(xb)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            val_loss += loss_fn(pred, yb).item() * len(xb)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.6f} | val MSE {val_loss:.6f}\")\n",
    "\n",
    "# load best\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Best val MSE:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a60e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UL Baseline DNN (B→B) Model 1 ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.101165\n",
      "RMSE : 0.318064\n",
      "MAE  : 0.226389\n",
      "MRE% : 1.9682\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy().reshape(-1)\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(yb.numpy().reshape(-1))\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mre  = mean_relative_error(y_true, y_pred)\n",
    "\n",
    "print(\"=== UL Baseline DNN (B→B) Model 1 ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aec25e",
   "metadata": {},
   "source": [
    "## Model 2: Regularised Deep Neural Network (DNN)\n",
    "\n",
    "To address the overfitting observed in the baseline model, we implement a regularised DNN architecture following the Cam-Ready paper design.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- Input layer: number of neurons = number of input features  \n",
    "- Hidden Layer 1: 128 neurons  \n",
    "- Hidden Layer 2: 64 neurons  \n",
    "- Hidden Layer 3: 32 neurons  \n",
    "- Output layer: 1 neuron (predicting continuous power consumption)\n",
    "\n",
    "### Regularisation Techniques\n",
    "\n",
    "- ReLU activation\n",
    "- Batch Normalisation before activation\n",
    "- Dropout (p = 0.3) after each hidden layer\n",
    "- L2 weight decay (λ = 0.01)\n",
    "- Adam optimizer\n",
    "- MSE loss function\n",
    "- Batch size = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb36c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RegularizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegularizedDNN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "X_train_tensor = torch.FloatTensor(X_train_s)\n",
    "X_test_tensor  = torch.FloatTensor(X_test_s)\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1,1)\n",
    "y_test_tensor  = torch.FloatTensor(y_test).view(-1,1)\n",
    "\n",
    "input_dim = X_train_s.shape[1]\n",
    "\n",
    "model2 = RegularizedDNN(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model2.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01   # L2 regularisation\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc34166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UL dataset (model2) training:\n",
      "Epoch 001 | train MSE 110.952705 | val MSE 110.755653\n",
      "Epoch 010 | train MSE 0.650688 | val MSE 0.619446\n",
      "Epoch 020 | train MSE 0.344115 | val MSE 0.303138\n",
      "Epoch 030 | train MSE 0.363471 | val MSE 0.337512\n",
      "Epoch 040 | train MSE 0.279739 | val MSE 0.276426\n",
      "Epoch 050 | train MSE 0.242075 | val MSE 0.214753\n",
      "Epoch 060 | train MSE 0.315904 | val MSE 0.280791\n",
      "Epoch 070 | train MSE 0.200664 | val MSE 0.172907\n",
      "Epoch 080 | train MSE 0.282355 | val MSE 0.250495\n",
      "Epoch 090 | train MSE 0.264489 | val MSE 0.260440\n",
      "Epoch 100 | train MSE 0.279823 | val MSE 0.271966\n",
      "Epoch 110 | train MSE 0.204197 | val MSE 0.192564\n",
      "Epoch 120 | train MSE 0.229644 | val MSE 0.218446\n",
      "Epoch 130 | train MSE 0.158343 | val MSE 0.139926\n",
      "Epoch 140 | train MSE 0.143005 | val MSE 0.128126\n",
      "Epoch 150 | train MSE 0.128454 | val MSE 0.110733\n",
      "Epoch 160 | train MSE 0.120743 | val MSE 0.101953\n",
      "Epoch 170 | train MSE 0.110206 | val MSE 0.093343\n",
      "Epoch 180 | train MSE 0.121582 | val MSE 0.102202\n",
      "Epoch 190 | train MSE 0.106747 | val MSE 0.089998\n",
      "Epoch 200 | train MSE 0.117158 | val MSE 0.105789\n",
      "Best val MSE: 0.08866564184427261\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "# ---- add val tensors (no logic change, just for printing val MSE) ----\n",
    "X_val_tensor = torch.FloatTensor(X_val_s)\n",
    "y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "\n",
    "print(\"\\nUL dataset (model2) training:\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    permutation = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = X_train_tensor[indices]\n",
    "        batch_y = y_train_tensor[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- epoch-end evaluation: train MSE & val MSE (like model1) ----\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = model2(X_train_tensor)\n",
    "        val_pred   = model2(X_val_tensor)\n",
    "\n",
    "        train_mse = criterion(train_pred, y_train_tensor).item()\n",
    "        val_mse   = criterion(val_pred, y_val_tensor).item()\n",
    "\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "\n",
    "    # print at the same rhythm as your model1 screenshot (every 10 epochs + epoch 1)\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1:03d} | train MSE {train_mse:.6f} | val MSE {val_mse:.6f}\")\n",
    "\n",
    "print(f\"Best val MSE: {best_val_mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f23bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 2: Regularised DNN ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.115565\n",
      "RMSE : 0.339948\n",
      "MAE  : 0.242979\n",
      "MRE% : 2.0829\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model2(X_test_tensor)\n",
    "\n",
    "y_pred = y_pred.numpy().flatten()\n",
    "y_true = y_test_tensor.numpy().flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mre  = mean_relative_error(y_true, y_pred)\n",
    "\n",
    "print(\"\\n=== Model 2: Regularised DNN ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b1cb2",
   "metadata": {},
   "source": [
    "## Model 3: Hybrid DNN–XGBoost (DNN Feature Extractor + XGBoost Regressor)\n",
    "\n",
    "This model follows the hybrid pipeline described in the Cam-Ready paper.  \n",
    "The key idea is to split the learning process into two stages:\n",
    "\n",
    "1) A DNN is trained as a **feature extractor** to learn compact latent representations.  \n",
    "2) The DNN is frozen, and a separate **XGBoost regressor** is trained on the extracted embeddings.\n",
    "\n",
    "### DNN Feature Extractor Architecture\n",
    "- Dense layers: 587 → 261 → 186 → 99\n",
    "- Bottleneck embedding layer: 16 neurons\n",
    "- Output head (for DNN training): 1 neuron (MSE loss)\n",
    "\n",
    "### XGBoost Regressor (trained on embeddings)\n",
    "- max_depth = 5\n",
    "- n_estimators = 256\n",
    "- learning_rate = 0.22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e633bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class HybridFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN feature extractor + a small regression head.\n",
    "    We train this end-to-end first, then freeze and use the 16-dim embeddings for XGBoost.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 587),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(587, 261),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(261, 186),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(186, 99),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(99, 16)   # bottleneck embeddings (paper uses 16)\n",
    "        )\n",
    "\n",
    "        # regression head for training the DNN stage\n",
    "        self.reg_head = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.feature_net(x)\n",
    "        out = self.reg_head(emb)\n",
    "        return out, emb\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_s)\n",
    "X_test_tensor  = torch.FloatTensor(X_test_s)\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_test_tensor  = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "input_dim = X_train_s.shape[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UL dataset (model3 - Hybrid DNN) training:\n",
      "Epoch 001 | train MSE 2.134309 | val MSE 2.009862\n",
      "Epoch 010 | train MSE 0.100612 | val MSE 0.092106\n",
      "Epoch 020 | train MSE 0.098879 | val MSE 0.092511\n",
      "Epoch 030 | train MSE 0.117194 | val MSE 0.104905\n",
      "Epoch 040 | train MSE 0.092440 | val MSE 0.086033\n",
      "Epoch 050 | train MSE 0.094532 | val MSE 0.089535\n",
      "Epoch 060 | train MSE 0.095688 | val MSE 0.086954\n",
      "Epoch 070 | train MSE 0.150804 | val MSE 0.150639\n",
      "Epoch 080 | train MSE 0.130454 | val MSE 0.127588\n",
      "Epoch 090 | train MSE 0.085515 | val MSE 0.079729\n",
      "Epoch 100 | train MSE 0.089174 | val MSE 0.082029\n",
      "Epoch 110 | train MSE 0.108990 | val MSE 0.101705\n",
      "Epoch 120 | train MSE 0.107719 | val MSE 0.098780\n",
      "Epoch 130 | train MSE 0.087650 | val MSE 0.083525\n",
      "Epoch 140 | train MSE 0.172240 | val MSE 0.166170\n",
      "Epoch 150 | train MSE 0.094459 | val MSE 0.089151\n",
      "Epoch 160 | train MSE 0.144763 | val MSE 0.135806\n",
      "Epoch 170 | train MSE 0.079334 | val MSE 0.074968\n",
      "Epoch 180 | train MSE 0.084880 | val MSE 0.081317\n",
      "Epoch 190 | train MSE 0.090992 | val MSE 0.086750\n",
      "Epoch 200 | train MSE 0.088279 | val MSE 0.085414\n",
      "Best val MSE: 0.07476461678743362\n",
      "Embeddings shape (train): (4153, 16)\n",
      "Embeddings shape (test) : (1154, 16)\n"
     ]
    }
   ],
   "source": [
    "model3_dnn = HybridFeatureExtractor(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model3_dnn.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "\n",
    "print(\"\\nUL dataset (model3 - Hybrid DNN) training:\")\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_s)\n",
    "y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model3_dnn.train()\n",
    "\n",
    "    perm = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        bx = X_train_tensor[idx]\n",
    "        by = y_train_tensor[idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model3_dnn(bx)\n",
    "        loss = criterion(pred, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- epoch-end evaluation ----\n",
    "    model3_dnn.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred, _ = model3_dnn(X_train_tensor)\n",
    "        val_pred, _   = model3_dnn(X_val_tensor)\n",
    "\n",
    "        train_mse = criterion(train_pred, y_train_tensor).item()\n",
    "        val_mse   = criterion(val_pred, y_val_tensor).item()\n",
    "\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1:03d} | train MSE {train_mse:.6f} | val MSE {val_mse:.6f}\")\n",
    "\n",
    "print(f\"Best val MSE: {best_val_mse}\")\n",
    "\n",
    "model3_dnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, emb_train = model3_dnn(X_train_tensor)\n",
    "    _, emb_test  = model3_dnn(X_test_tensor)\n",
    "\n",
    "emb_train = emb_train.numpy()\n",
    "emb_test  = emb_test.numpy()\n",
    "\n",
    "print(\"Embeddings shape (train):\", emb_train.shape)\n",
    "print(\"Embeddings shape (test) :\", emb_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92973642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model 3: Hybrid DNN–XGBoost ===\n",
      "X: ['airtime', 'selected_mcs', 'txgain']  y: pm_power\n",
      "MSE  : 0.103988\n",
      "RMSE : 0.322472\n",
      "MAE  : 0.225786\n",
      "MRE% : 1.9629\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    n_estimators=256,\n",
    "    learning_rate=0.22,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(emb_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(emb_test).reshape(-1)\n",
    "y_true = np.asarray(y_test).reshape(-1)\n",
    "\n",
    "def mean_relative_error(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100)\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred_xgb)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = mean_absolute_error(y_true, y_pred_xgb)\n",
    "mre  = mean_relative_error(y_true, y_pred_xgb)\n",
    "\n",
    "print(\"\\n=== Model 3: Hybrid DNN–XGBoost ===\")\n",
    "print(\"X:\", feature_cols, \" y:\", target_col)\n",
    "print(f\"MSE  : {mse:.6f}\")\n",
    "print(f\"RMSE : {rmse:.6f}\")\n",
    "print(f\"MAE  : {mae:.6f}\")\n",
    "print(f\"MRE% : {mre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c269bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lens: 1154 1154 1154 1154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ===== Model 1 predictions =====\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    m1_pred_ul = model(X_test_tensor.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "# ===== Model 2 predictions =====\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    m2_pred_ul = model2(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# ===== Model 3 predictions (Hybrid DNN + XGB) =====\n",
    "model3_dnn.eval()\n",
    "with torch.no_grad():\n",
    "    _, emb_test = model3_dnn(X_test_tensor)\n",
    "\n",
    "emb_test = emb_test.cpu().numpy()\n",
    "m3_pred_ul = xgb.predict(emb_test).reshape(-1)\n",
    "\n",
    "# ===== sanity check =====\n",
    "print(\"lens:\", len(y_true_ul), len(m1_pred_ul), len(m2_pred_ul), len(m3_pred_ul))\n",
    "assert len(y_true_ul)==len(m1_pred_ul)==len(m2_pred_ul)==len(m3_pred_ul), \"❌ Length mismatch: predictions not aligned!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca86a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UL Dataset Overall Comparison ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MRE (%)</th>\n",
       "      <th>Mean Power (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>0.101165</td>\n",
       "      <td>0.318064</td>\n",
       "      <td>0.226389</td>\n",
       "      <td>1.968227</td>\n",
       "      <td>11.461376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>0.115565</td>\n",
       "      <td>0.339948</td>\n",
       "      <td>0.242979</td>\n",
       "      <td>2.082883</td>\n",
       "      <td>11.371135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>0.103988</td>\n",
       "      <td>0.322472</td>\n",
       "      <td>0.225786</td>\n",
       "      <td>1.962944</td>\n",
       "      <td>11.456969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model       MSE      RMSE       MAE   MRE (%)  Mean Power (W)\n",
       "0  Model 1  0.101165  0.318064  0.226389  1.968227       11.461376\n",
       "1  Model 2  0.115565  0.339948  0.242979  2.082883       11.371135\n",
       "2  Model 3  0.103988  0.322472  0.225786  1.962944       11.456969"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental Mean Power (W): 11.4437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def mre_percent(y_true, y_pred, eps=1e-9):\n",
    "    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + eps)) * 100\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mre  = mre_percent(y_true, y_pred)\n",
    "    mean_power = np.mean(y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MRE (%)\": mre,\n",
    "        \"Mean Power (W)\": mean_power\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append(evaluate_model(\"Model 1\", y_true_ul, m1_pred_ul))\n",
    "results.append(evaluate_model(\"Model 2\", y_true_ul, m2_pred_ul))\n",
    "results.append(evaluate_model(\"Model 3\", y_true_ul, m3_pred_ul))\n",
    "\n",
    "true_mean_power = np.mean(y_true_ul)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results = df_results.round(6)\n",
    "\n",
    "print(\"\\n=== UL Dataset Overall Comparison ===\")\n",
    "display(df_results)\n",
    "\n",
    "print(\"\\nExperimental Mean Power (W):\", round(true_mean_power, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oran311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
